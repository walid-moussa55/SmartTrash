import pandas as pd
import numpy as np
import os
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from tslearn.clustering import TimeSeriesKMeans
from tslearn.preprocessing import TimeSeriesScalerMeanVariance
from hmmlearn import hmm
import warnings
warnings.filterwarnings('ignore')

class SmartTrashPatternAnalyzer:
    def __init__(self, raw_data):
        """
        Analyseur de patterns d'usage pour poubelles intelligentes
        """
        self.data = self.load_data(raw_data)
        self.processed_data = None
        self.time_series_data = None
        self.clusters = None
        self.hmm_model = None

    def load_data(self, raw_data):
        """Charger et pr√©processer les donn√©es JSON"""
        try:
            
            # Convertir en DataFrame
            df = pd.DataFrame(raw_data)

            # Traitement des colonnes MongoDB
            if '_id' in df.columns:
                df['id'] = df['_id'].apply(lambda x: x.get('$oid', x) if isinstance(x, dict) else x)
                df.drop('_id', axis=1, inplace=True)

            if 'timestamp' in df.columns:
                df['timestamp'] = df['timestamp'].apply(
                    lambda x: pd.to_datetime(x.get('$date', x) if isinstance(x, dict) else x)
                )

            # Trier par timestamp
            df = df.sort_values('timestamp').reset_index(drop=True)

            print(f"Donn√©es charg√©es: {len(df)} enregistrements")
            print(f"P√©riode: {df['timestamp'].min()} √† {df['timestamp'].max()}")
            print(f"Poubelles uniques: {df['bin_id'].nunique()}")

            return df

        except Exception as e:
            print(f"Erreur lors du chargement: {e}")
            return None

    def feature_engineering(self):
        """Ing√©nierie des caract√©ristiques temporelles"""
        df = self.data.copy()

        # Caract√©ristiques temporelles
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.dayofweek
        df['day_of_month'] = df['timestamp'].dt.day
        df['month'] = df['timestamp'].dt.month
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

        # Cat√©gorisation des p√©riodes
        def categorize_time(hour):
            if 6 <= hour < 12:
                return 'morning'
            elif 12 <= hour < 18:
                return 'afternoon'
            elif 18 <= hour < 22:
                return 'evening'
            else:
                return 'night'

        df['time_period'] = df['hour'].apply(categorize_time)

        # Calcul des changements de level
        df = df.sort_values(['bin_id', 'timestamp'])
        df['trash_level_change'] = df.groupby('bin_id')['trash_level'].diff().fillna(0)
        df['usage_intensity'] = df['trash_level_change'].apply(lambda x: max(0, x))

        # Indicateurs d'activit√©
        df['is_active'] = (df['usage_intensity'] > 5).astype(int)
        df['high_usage'] = (df['usage_intensity'] > 20).astype(int)

        self.processed_data = df
        # print("Ing√©nierie des caract√©ristiques termin√©e")
        return df

    def create_time_series_profiles(self):
        """Cr√©er des profils de s√©ries temporelles pour chaque poubelle"""
        if self.processed_data is None:
            self.feature_engineering()

        profiles = {}

        for bin_id in self.processed_data['bin_id'].unique():
            bin_data = self.processed_data[self.processed_data['bin_id'] == bin_id].copy()

            # Profil horaire moyen
            hourly_profile = bin_data.groupby('hour').agg({
                'usage_intensity': 'mean',
                'trash_level': 'mean',
                'is_active': 'mean'
            }).values

            # Profil hebdomadaire
            weekly_profile = bin_data.groupby('day_of_week').agg({
                'usage_intensity': 'mean',
                'trash_level': 'mean',
                'is_active': 'mean'
            }).values

            # Profil par p√©riode
            period_profile = bin_data.groupby('time_period').agg({
                'usage_intensity': 'mean',
                'trash_level': 'mean',
                'is_active': 'mean'
            }).values

            # Extract latitude and longitude from the 'location' dictionary
            location_data = bin_data['location'].iloc[0]
            latitude = location_data.get('latitude', None)
            longitude = location_data.get('longitude', None)


            profiles[bin_id] = {
                'hourly': hourly_profile,
                'weekly': weekly_profile,
                'period': period_profile,
                'location': [latitude, longitude],
                'name': bin_data['name'].iloc[0],
                'trash_type': bin_data['trash_type'].iloc[0]
            }

        return profiles

    def time_series_clustering(self, n_clusters=5):
        """Clustering des s√©ries temporelles"""
        profiles = self.create_time_series_profiles()

        # Pr√©parer les donn√©es pour le clustering
        hourly_series = []
        bin_ids = []

        for bin_id, profile in profiles.items():
            hourly_series.append(profile['hourly'][:, 0])  # usage_intensity
            bin_ids.append(bin_id)

        # Normalisation
        scaler = TimeSeriesScalerMeanVariance()
        hourly_series_scaled = scaler.fit_transform(hourly_series)

        # Clustering avec TimeSeriesKMeans
        model = TimeSeriesKMeans(n_clusters=n_clusters, metric="euclidean", random_state=42)
        cluster_labels = model.fit_predict(hourly_series_scaled)

        # R√©sultats
        results = pd.DataFrame({
            'bin_id': bin_ids,
            'cluster': cluster_labels
        })

        # Ajouter les informations des poubelles
        for i, (bin_id, profile) in enumerate(profiles.items()):
            results.loc[results['bin_id'] == bin_id, 'name'] = profile['name']
            results.loc[results['bin_id'] == bin_id, 'trash_type'] = profile['trash_type']
            results.loc[results['bin_id'] == bin_id, 'latitude'] = profile['location'][0]
            results.loc[results['bin_id'] == bin_id, 'longitude'] = profile['location'][1]


        self.clusters = results
        self.time_series_data = {
            'profiles': profiles,
            'series': hourly_series_scaled,
            'model': model,
            'scaler': scaler
        }

        return results

    def hidden_markov_analysis(self, n_states=4):
        """Analyse avec mod√®les de Markov cach√©s"""
        if self.processed_data is None:
            self.feature_engineering()

        # Pr√©parer les s√©quences d'√©tats pour chaque poubelle
        hmm_results = {}

        for bin_id in self.processed_data['bin_id'].unique():
            bin_data = self.processed_data[self.processed_data['bin_id'] == bin_id].copy()

            # Cr√©er des √©tats bas√©s sur l'intensit√© d'usage
            features = bin_data[['usage_intensity', 'trash_level', 'is_active']].values

            # Normalisation
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)

            # Mod√®le HMM
            model = hmm.GaussianHMM(n_components=n_states, random_state=42)

            try:
                model.fit(features_scaled)
                states = model.predict(features_scaled)

                # Analyse des √©tats
                state_analysis = {}
                for state in range(n_states):
                    state_mask = states == state
                    if state_mask.sum() > 0:
                        state_analysis[f'state_{state}'] = {
                            'frequency': state_mask.mean(),
                            'avg_usage': bin_data.loc[state_mask, 'usage_intensity'].mean(),
                            'avg_level': bin_data.loc[state_mask, 'trash_level'].mean(),
                            'most_common_period': bin_data.loc[state_mask, 'time_period'].mode().iloc[0] if len(bin_data.loc[state_mask, 'time_period'].mode()) > 0 else 'unknown'
                        }

                hmm_results[bin_id] = {
                    'model': model,
                    'states': states,
                    'state_analysis': state_analysis,
                    'scaler': scaler
                }

            except Exception as e:
                print(f"Erreur HMM pour {bin_id}: {e}")
                continue

        self.hmm_model = hmm_results
        return hmm_results

    def analyze_usage_patterns(self):
        """Analyse compl√®te des patterns d'usage"""
        print("üîç Analyse des Patterns d'Usage - Poubelles Intelligentes")
        # print("=" * 60)

        # 1. Clustering des s√©ries temporelles
        # print("\n1. Clustering des profils temporels...")
        clusters = self.time_series_clustering()

        # 2. Analyse HMM
        # print("\n2. Analyse des √©tats cach√©s (HMM)...")
        hmm_results = self.hidden_markov_analysis()

        # 3. Analyse des patterns
        # print("\n3. R√©sum√© des patterns identifi√©s:")
        # print("-" * 40)

        # Patterns par cluster
        for cluster_id in clusters['cluster'].unique():
            cluster_bins = clusters[clusters['cluster'] == cluster_id]
            # print(f"\nüìä Cluster {cluster_id} ({len(cluster_bins)} poubelles):")
            # print(f"   Types de d√©chets: {cluster_bins['trash_type'].value_counts().to_dict()}")
            # print(f"   Poubelles: {', '.join(cluster_bins['name'].tolist())}")

        # Patterns temporels globaux
        # print(f"\nüïê Patterns temporels globaux:")
        hourly_usage = self.processed_data.groupby('hour')['usage_intensity'].mean()
        peak_hour = hourly_usage.idxmax()
        # print(f"   Heure de pic: {peak_hour}h (intensit√©: {hourly_usage[peak_hour]:.1f})")

        daily_usage = self.processed_data.groupby('day_of_week')['usage_intensity'].mean()
        days = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']
        peak_day = daily_usage.idxmax()
        # print(f"   Jour de pic: {days[peak_day]} (intensit√©: {daily_usage[peak_day]:.1f})")

        weekend_vs_weekday = self.processed_data.groupby('is_weekend')['usage_intensity'].mean()
        # print(f"   Usage weekend vs semaine: {weekend_vs_weekday[1]:.1f} vs {weekend_vs_weekday[0]:.1f}")

        return {
            'clusters': clusters,
            'hmm_results': hmm_results,
            'temporal_patterns': {
                'hourly': hourly_usage.to_dict(),
                'daily': daily_usage.to_dict(),
                'weekend_vs_weekday': weekend_vs_weekday.to_dict()
            }
        }

    def generate_insights(self):
        """G√©n√©rer des insights pour les politiques environnementales"""
        results = self.analyze_usage_patterns()

        insights = {
            'operational_insights': [],
            'environmental_policies': [],
            'optimization_opportunities': []
        }

        # Insights op√©rationnels
        clusters = results['clusters']
        for cluster_id in clusters['cluster'].unique():
            cluster_bins = clusters[clusters['cluster'] == cluster_id]
            if len(cluster_bins) > 1:
                insights['operational_insights'].append(
                    f"Cluster {cluster_id}: {len(cluster_bins)} poubelles avec des patterns similaires - "
                    f"optimisation possible des tourn√©es de collecte"
                )

        # Insights environnementaux
        temporal = results['temporal_patterns']
        peak_hour = max(temporal['hourly'], key=temporal['hourly'].get)
        insights['environmental_policies'].append(
            f"Pic d'usage √† {peak_hour}h - sensibilisation cibl√©e possible"
        )

        # Diff√©rences types de d√©chets
        type_usage = self.processed_data.groupby('trash_type')['usage_intensity'].mean()
        if len(type_usage) > 1:
            max_type = type_usage.idxmax()
            min_type = type_usage.idxmin()
            insights['environmental_policies'].append(
                f"Type '{max_type}' le plus utilis√© ({type_usage[max_type]:.1f}) vs "
                f"'{min_type}' ({type_usage[min_type]:.1f}) - campagne de sensibilisation cibl√©e"
            )

        # Opportunit√©s d'optimisation
        if temporal['weekend_vs_weekday'][1] > temporal['weekend_vs_weekday'][0]:
            insights['optimization_opportunities'].append(
                "Usage plus intense le weekend - ajuster la fr√©quence de collecte"
            )
        else:
            insights['optimization_opportunities'].append(
                "Usage plus faible le weekend - r√©duire la fr√©quence de collecte weekend"
            )

        return insights


    def generate_dynamic_report(self):
        """G√©n√©rer un rapport complet adapt√© aux donn√©es sp√©cifiques"""
        # Analyser les donn√©es
        results = self.analyze_usage_patterns()

        # G√©n√©rer des insights bas√©s sur les r√©sultats de l'analyse
        insights = self.generate_insights()

        # Statistiques de base
        stats = self.get_basic_statistics()

        # Construire le rapport dynamique
        report = self.build_adaptive_report(results, insights, stats)

        return report


    def get_basic_statistics(self):
        """Calculer les statistiques de base des donn√©es"""
        if self.processed_data is None:
            self.feature_engineering()

        # Extract latitude and longitude from the 'location' dictionary before grouping
        self.data['latitude'] = self.data['location'].apply(lambda x: x.get('latitude', None))
        self.data['longitude'] = self.data['location'].apply(lambda x: x.get('longitude', None))


        stats = {
            'total_records': len(self.data),
            'total_bins': self.data['bin_id'].nunique(),
            'date_range': {
                'start': self.data['timestamp'].min(),
                'end': self.data['timestamp'].max(),
                'days': (self.data['timestamp'].max() - self.data['timestamp'].min()).days
            },
            'trash_types': self.data['trash_type'].value_counts().to_dict(),
            'locations': self.data.groupby('bin_id').agg({
                'name': 'first',
                'latitude': 'first',
                'longitude': 'first',
                'trash_type': 'first'
            }).to_dict('index'),
            'usage_stats': {
                'max_intensity': self.processed_data['usage_intensity'].max(),
                'avg_intensity': self.processed_data['usage_intensity'].mean(),
                'total_usage_events': (self.processed_data['usage_intensity'] > 0).sum()
            }
        }

        return stats

    def build_adaptive_report(self, results, insights, stats):
        """Construire un rapport adapt√© aux r√©sultats sp√©cifiques"""

        # En-t√™te du rapport
        report = f"""# üåç Rapport d'Analyse des Patterns d'Usage - Poubelles Intelligentes

## **Vue d'Ensemble des Donn√©es**

**P√©riode d'analyse**: {stats['date_range']['start'].strftime('%d/%m/%Y')} au {stats['date_range']['end'].strftime('%d/%m/%Y')} ({stats['date_range']['days']} jours)
**Nombre d'enregistrements**: {stats['total_records']:,}
**Poubelles analys√©es**: {stats['total_bins']}
**√âv√©nements d'usage d√©tect√©s**: {stats['usage_stats']['total_usage_events']:,}

### **R√©partition par Type de D√©chets**
"""

        # Ajout dynamique des types de d√©chets
        for trash_type, count in stats['trash_types'].items():
            percentage = (count / stats['total_bins']) * 100
            report += f"- **{trash_type.capitalize()}**: {count} poubelles ({percentage:.1f}%)\n"

        # Analyse des patterns temporels
        temporal = results['temporal_patterns']
        peak_hour = max(temporal['hourly'], key=temporal['hourly'].get)
        peak_intensity = temporal['hourly'][peak_hour]

        days_fr = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']
        peak_day_idx = max(temporal['daily'], key=temporal['daily'].get)
        peak_day = days_fr[peak_day_idx]
        peak_day_intensity = temporal['daily'][peak_day_idx]

        weekend_intensity = temporal['weekend_vs_weekday'][1]
        weekday_intensity = temporal['weekend_vs_weekday'][0]

        report += f"""
## **Patterns Temporels Identifi√©s**

### **Rythmes de Consommation D√©tect√©s**

| Pattern D√©tect√© | Valeur | Impact Comportemental |
|-----------------|--------|----------------------|
| **Heure de pic** | {peak_hour}h (intensit√©: {peak_intensity:.1f}) | Pause matinale g√©n√©ralis√©e |
| **Jour de pic** | {peak_day} (intensit√©: {peak_day_intensity:.1f}) | {"Pr√©paration weekend" if peak_day_idx == 3 else "Activit√© hebdomadaire sp√©cifique"} |
| **Weekend vs Semaine** | {weekend_intensity:.1f} vs {weekday_intensity:.1f} | {"R√©duction activit√© weekend" if weekend_intensity < weekday_intensity else "Intensification weekend"} |

"""

        # Analyse des clusters - Adaptation dynamique
        clusters = results['clusters']
        unique_clusters = clusters['cluster'].nunique()

        report += f"""## **Analyse des Profils d'Usage ({unique_clusters} clusters identifi√©s)**

"""

        for cluster_id in sorted(clusters['cluster'].unique()):
            cluster_bins = clusters[clusters['cluster'] == cluster_id]
            cluster_size = len(cluster_bins)
            trash_types_in_cluster = cluster_bins['trash_type'].value_counts().to_dict()

            # D√©terminer le profil du cluster
            if cluster_size == 1:
                profile_type = "**Profil Unique**"
                behavioral_insight = "Comportement sp√©cialis√© n√©cessitant une approche individualis√©e"
            elif cluster_size <= 3:
                profile_type = "**Profil Minoritaire**"
                behavioral_insight = "Groupe restreint avec des habitudes sp√©cifiques"
            else:
                profile_type = "**Profil Majoritaire**"
                behavioral_insight = "Comportement collectif synchronis√© - levier de changement important"

            report += f"""### **Cluster {cluster_id}** - {profile_type}

**Composition**: {cluster_size} poubelles
**Types de d√©chets**: {', '.join([f"{k}({v})" for k, v in trash_types_in_cluster.items()])}
**Insight comportemental**: {behavioral_insight}

**Poubelles concern√©es**:
"""
            for _, bin_info in cluster_bins.iterrows():
                report += f"- {bin_info['name']} ({bin_info['trash_type']})\n"

            report += "\n"

        # Politiques recommandes - Adaptation aux r√©sultats
        report += """## **Politiques Environnementales Recommand√©es**

### **A. Sensibilisation Cibl√©e Temporelle**

"""

        # Strat√©gie basee sur l'heure de pic
        if 8 <= peak_hour <= 11:
            strategy_name = "Pause Verte Matinale"
            strategy_action = "Promotion des contenants r√©utilisables pendant les pauses caf√©"
            strategy_target = "R√©duction des d√©chets d'emballage matinaux"
        elif 12 <= peak_hour <= 14:
            strategy_name = "D√©jeuner Responsable"
            strategy_action = "Incitation aux repas sans emballage jetable"
            strategy_target = "Diminution des d√©chets alimentaires"
        elif 17 <= peak_hour <= 19:
            strategy_name = "Fin de Journ√©e √âco"
            strategy_action = "Sensibilisation anti-gaspillage en fin d'activit√©"
            strategy_target = "R√©duction de l'accumulation de d√©chets"
        else:
            strategy_name = "Intervention Horaire Cibl√©e"
            strategy_action = "Campagne sp√©cifique √† l'heure de pic identifi√©e"
            strategy_target = "Optimisation du moment d'impact maximal"

        report += f"""#### **Strat√©gie "{strategy_name}"**
- **Objectif**: {strategy_target}
- **Action**: {strategy_action}
- **Horaire cibl√©**: {peak_hour-1}h-{peak_hour+1}h
- **KPI**: R√©duction de 20% de l'intensit√© d'usage pendant cette p√©riode

"""

        # Strat√©gie bas√©e sur le jour de pic
        if peak_day_idx == 3:  # Jeudi
            day_strategy = """#### **Strat√©gie "Jeudi Vert"**
- **Objectif**: R√©duire le pic de consommation pr√©-weekend
- **Action**: Campagne "Pr√©parez votre weekend responsable"
- **Mesure**: Ateliers de sensibilisation le mercredi
- **KPI**: R√©duction de 15% du pic du jeudi
"""
        elif peak_day_idx in [5, 6]:  # Weekend
            day_strategy = """#### **Strat√©gie "Weekend Durable"**
- **Objectif**: Optimiser l'usage weekend
- **Action**: Promotion d'activit√©s √©co-responsables
- **Mesure**: Partenariats avec commerces locaux durables
- **KPI**: Maintien de l'engagement environnemental hors p√©riode de travail
"""
        else:
            day_strategy = f"""#### **Strat√©gie "Optimisation {peak_day}"**
- **Objectif**: Comprendre et r√©duire le pic du {peak_day.lower()}
- **Action**: √âtude comportementale cibl√©e
- **Mesure**: Intervention personnalis√©e selon les causes identifi√©es
- **KPI**: Lissage de la courbe d'usage hebdomadaire
"""

        report += day_strategy

        # Politiques par type de d√©chet - Adaptation dynamique
        type_usage = self.processed_data.groupby('trash_type')['usage_intensity'].mean().sort_values(ascending=False)
        max_type = type_usage.index[0]
        max_value = type_usage.iloc[0]

        if len(type_usage) > 1:
            min_type = type_usage.index[-1]
            min_value = type_usage.iloc[-1]
        else:
            min_type = "aucun autre"
            min_value = 0

        report += f"""### **B. Politiques de R√©duction par Type de D√©chet**

#### **Focus Anti-{max_type.capitalize()}**
**Constat**: {max_type.capitalize()} = type le plus probl√©matique (intensit√©: {max_value:.1f})

- **Politique**: {"Taxe sur emballages plastique" if max_type == "plastic" else f"R√©duction incitative des d√©chets {max_type}"}
- **Alternative**: {"Emballages biod√©gradables" if max_type == "plastic" else f"Solutions r√©utilisables pour {max_type}"}
- **√âducation**: "D√©fi sans {max_type}" pendant les heures de pic

"""

        # Optimisation logistique basee sur les patterns
        if weekend_intensity < weekday_intensity:
            logistics_strategy = """### **C. Optimisation Logistique**

#### **Collecte Intelligente Adapt√©e**
- **Lundi-Mercredi**: Collecte standard
- **Jeudi-Vendredi**: Collecte renforc√©e (anticipation weekend)
- **Weekend**: Collecte r√©duite (usage plus faible d√©tect√©)
- **√âconomie attendue**: 25-30% de r√©duction des trajets inutiles
"""
        else:
            logistics_strategy = """### **C. Optimisation Logistique**

#### **Collecte Weekend Renforc√©e**
- **Semaine**: Collecte standard
- **Weekend**: Collecte renforc√©e (usage intensifi√© d√©tect√©)
- **Adaptation**: √âquipes weekend sp√©cialis√©es
- **Optimisation**: Couverture adapt√©e aux patterns d'usage weekend
"""

        report += logistics_strategy


        # Plan d'implementation adapte
        cluster_count = unique_clusters
        bin_count = stats['total_bins']

        if cluster_count <= 2:
            implementation_complexity = "Simple"
            timeline = "3-4 mois"
        elif cluster_count <= 4:
            implementation_complexity = "Mod√©r√©e"
            timeline = "4-6 mois"
        else:
            implementation_complexity = "Complexe"
            timeline = "6-8 mois"


        report += f"""## **Plan d'Impl√©mentation ({implementation_complexity})**

**Dur√©e estim√©e**: {timeline}
**Complexit√©**: {implementation_complexity} ({cluster_count} profils comportementaux identifi√©s)

### **Phase 1 - Sensibilisation (Mois 1-2)**
- Campagnes cibl√©es aux heures de pic ({peak_hour}h) et jour de pic ({peak_day})
- Installation signal√©tique adapt√©e aux {cluster_count} profils identifi√©s
- Lancement application mobile avec recommandations personnalis√©es

### **Phase 2 - Infrastructure (Mois 3-4)**
- Optimisation placement des {bin_count} poubelles selon les clusters
- Installation d'alternatives √©co-responsables (fontaines, stations de tri)
- Mise en place de la collecte intelligente adapt√©e aux patterns

### **Phase 3 - Gamification et Suivi (Mois 5-6)**
- Challenges collectifs anti-{max_type}
- Syst√®me de r√©compenses bas√© sur les am√©liorations mesur√©es
- Tableaux de bord communautaires temps r√©el

## **Indicateurs de Performance Adapt√©s**

| Objectif | Valeur Actuelle | Cible {timeline.split('-')[0]} mois | Impact Attendu |
|----------|-----------------|------------|----------------|
| R√©duction pic {peak_day} | {peak_day_intensity:.1f} intensit√© | {peak_day_intensity*0.85:.1f} intensit√© | -15% |
| R√©duction {max_type} | {max_value:.1f} usage | {max_value*0.7:.1f} usage | -30% |
| Optimisation collecte | 7j/7 | {"5j/7" if weekend_intensity < weekday_intensity else "Collecte adapt√©e"} | -25% trajets |
| Efficacit√© par cluster | {cluster_count} profils | Harmonisation | +40% ciblage |


## **Impact Environnemental Projet√©**

### **Court terme (6 mois)**
- **R√©duction d√©chets**: -20% gr√¢ce aux interventions cibl√©es
- **Optimisation logistique**: -25% √©missions transport
- **Engagement citoyen**: +60% participation aux programmes

### **Moyen terme (1-2 ans)**
- **Changement comportemental durable**: -35% d√©chets totaux
- **√âconomie circulaire**: +50% taux de recyclage
- **R√©plication mod√®le**: Extension √† d'autres zones urbaines

---

**Rapport g√©n√©r√© automatiquement le {datetime.now().strftime('%d/%m/%Y √† %H:%M')}**
*Bas√© sur l'analyse de {stats['total_records']:,} enregistrements de {stats['total_bins']} poubelles intelligentes*
"""

        return report

    def visualize_patterns(self):
        """Visualisation des patterns identifi√©s"""
        if self.processed_data is None:
            self.feature_engineering()

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Analyse des Patterns d\'Usage - Poubelles Intelligentes', fontsize=16)

        # 1. Heatmap usage par heure et jour
        pivot_data = self.processed_data.groupby(['hour', 'day_of_week'])['usage_intensity'].mean().unstack()
        sns.heatmap(pivot_data, ax=axes[0,0], cmap='YlOrRd', cbar_kws={'label': 'Intensit√© d\'usage'})
        axes[0,0].set_title('Heatmap: Usage par Heure et Jour')
        axes[0,0].set_xlabel('Jour de la semaine')
        axes[0,0].set_ylabel('Heure')

        # 2. Patterns par type de d√©chet
        type_hourly = self.processed_data.groupby(['trash_type', 'hour'])['usage_intensity'].mean().unstack()
        type_hourly.T.plot(ax=axes[0,1], marker='o')
        axes[0,1].set_title('Patterns d\'Usage par Type de D√©chet')
        axes[0,1].set_xlabel('Heure')
        axes[0,1].set_ylabel('Intensit√© d\'usage moyenne')
        axes[0,1].legend(title='Type de d√©chet')

        # 3. Distribution des niveaux par p√©riode
        self.processed_data.boxplot(column='trash_level', by='time_period', ax=axes[1,0])
        axes[1,0].set_title('Distribution des Niveaux par P√©riode')
        axes[1,0].set_xlabel('P√©riode de la journ√©e')
        axes[1,0].set_ylabel('Niveau de remplissage (%)')

        # 4. Clusters si disponibles
        if self.clusters is not None:
            cluster_summary = self.clusters.groupby('cluster').size()
            axes[1,1].pie(cluster_summary.values, labels=[f'Cluster {i}' for i in cluster_summary.index], autopct='%1.1f%%')
            axes[1,1].set_title('R√©partition des Poubelles par Cluster')
        else:
            axes[1,1].text(0.5, 0.5, 'Clustering non effectu√©', ha='center', va='center', transform=axes[1,1].transAxes)
            axes[1,1].set_title('Clusters')


        plt.tight_layout()
        plt.show()

        return fig

from others.database import MongoDB

def generate_patern_usage(raw_data):
    """Fonction principale pour g√©n√©rer les patterns d'usage"""
    
    # Cr√©ation de l'analyseur de patterns
    analyzer = SmartTrashPatternAnalyzer(raw_data)

    # Analyse compl√®te
    results = analyzer.analyze_usage_patterns()

    # G√©n√©ration d'insights
    insights = analyzer.generate_insights()

    # G√©n√©rer le rapport adapt√© aux donn√©es sp√©cifiques
    detailed_report = analyzer.generate_dynamic_report()

    # Sauvegarder le rapport
    report_filename = f"rapport_patterns_usage_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    report_filename = os.path.join('generated_files', report_filename)
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(detailed_report)

    print(f"‚úÖ Rapport d√©taill√© g√©n√©r√©: {report_filename}")
    return report_filename

